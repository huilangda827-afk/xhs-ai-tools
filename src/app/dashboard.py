# -*- coding: utf-8 -*-
"""
Streamlit Dashboard - Stage 4 (Enhanced)
AI Tools æ•°æ®æŒ–æ˜å·¥ä½œç«™

åŠŸèƒ½ï¼š
- æ•°æ®æºåˆ‡æ¢ï¼ˆCrawled / Sampleï¼‰
- Demo Mode æ ·ä¾‹åº“ç®¡ç†ï¼ˆè¦†ç›–/åˆå¹¶å»é‡ï¼‰
- ä¸€é”®æŒ–æ˜ï¼ˆå¸¦è¿›åº¦å’Œæ—¥å¿—ï¼‰
- å›¾è°±å¯è§†åŒ–
- PageRank Top æ¦œå•
- Rising Edges è¶‹åŠ¿æ¦œ
- åŸå¸–æ ·æœ¬å±•ç¤º
- ä¸€é”®å¯¼å‡ºæäº¤åŒ…
"""
import streamlit as st
import json
import os
import sys
import shutil
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.graph.builder import TagCooccurrenceGraph
from src.graph.analytics import GraphAnalytics
from src.graph.visualizer import GraphVisualizer
from src.utils.packaging import create_submission_package, deduplicate_jsonl, merge_jsonl_files
from src.app.components.insights import render_insights_panel

# æ¨¡æ¿å¼•æ“å»¶è¿Ÿå¯¼å…¥ï¼ˆå…œåº•æœºåˆ¶ï¼Œé¿å…æ–°æ¨¡å—æ‹–æŒ‚ç°æœ‰åŠŸèƒ½ï¼‰
try:
    from src.generator.template_engine import TemplateEngine, save_drafts_package
    TEMPLATE_ENGINE_AVAILABLE = True
    TEMPLATE_ENGINE_ERROR = None
except Exception as e:
    TemplateEngine = None
    save_drafts_package = None
    TEMPLATE_ENGINE_AVAILABLE = False
    TEMPLATE_ENGINE_ERROR = str(e)

# LLM å®¢æˆ·ç«¯å»¶è¿Ÿå¯¼å…¥ï¼ˆå¯é€‰åŠŸèƒ½ï¼Œå¤±è´¥ä¸å½±å“æ¨¡æ¿å¼•æ“ï¼‰
try:
    from src.generator.llm_client import generate_with_llm
    LLM_CLIENT_AVAILABLE = True
except Exception as e:
    generate_with_llm = None
    LLM_CLIENT_AVAILABLE = False


# é¡µé¢é…ç½®
st.set_page_config(
    page_title="AI Tools æ•°æ®æŒ–æ˜å·¥ä½œç«™",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS æ ·å¼
st.markdown("""
<style>
.big-font {
    font-size: 20px !important;
    font-weight: bold;
}
.metric-card {
    background-color: #f0f2f6;
    padding: 10px;
    border-radius: 5px;
    margin: 5px 0;
}
</style>
""", unsafe_allow_html=True)

# Session State åˆå§‹åŒ–ï¼ˆç¡®ä¿æ‰€æœ‰å…³é”®å˜é‡éƒ½æœ‰é»˜è®¤å€¼ï¼‰
for key, default in {
    "pagerank_top": [],
    "rising_edges": [],
    "window_stats": {},
    "graph_obj": None,
    "graph_path": None,
    "graph_nodes": 0,
    "graph_edges": 0,
    "items": [],
    "logs": [],
    "mine_done": False,
    "generated_drafts": [],
    "trigger_crawl": False,
    "crawl_keyword": "AIå·¥å…·",
    "crawl_count": 10,
}.items():
    if key not in st.session_state:
        st.session_state[key] = default

# æ ‡é¢˜
st.title("ğŸ¯ AI Tools æ•°æ®æŒ–æ˜å·¥ä½œç«™")
st.markdown("*Stage 1-4 Complete | ä»çˆ¬å–åˆ°å¯è§†åŒ–çš„å®Œæ•´æµç¨‹*")
st.markdown("---")


# ============= è¾…åŠ©å‡½æ•° =============

def load_jsonl(path):
    """åŠ è½½ JSONL æ–‡ä»¶"""
    items = []
    try:
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                items.append(json.loads(line.strip()))
    except FileNotFoundError:
        pass
    return items


def count_lines(path):
    """ç»Ÿè®¡æ–‡ä»¶è¡Œæ•°"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return sum(1 for _ in f)
    except FileNotFoundError:
        return 0


def save_jsonl(items, path):
    """ä¿å­˜ JSONL æ–‡ä»¶"""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        for item in items:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")


# ============= ä¾§è¾¹æ ï¼šæ§åˆ¶é¢æ¿ =============

with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶é¢æ¿")
    
    # === æ•°æ®æºé€‰æ‹© ===
    st.subheader("ğŸ“‚ æ•°æ®æº")
    data_source = st.radio(
        "é€‰æ‹©æ•°æ®æº",
        ["Crawled Data (çœŸå®æ•°æ®)", "Sample Data (æ¼”ç¤ºæ¨¡å¼)"],
        help="æ¼”ç¤ºæ¨¡å¼ä½¿ç”¨å†…ç½®æ ·ä¾‹ï¼Œç¡®ä¿ç¨³å®š"
    )
    
    # ç¡®å®šæ•°æ®è·¯å¾„
    if "Sample" in data_source:
        data_path = str(project_root / "data/samples/annotations_sample.jsonl")
        st.caption("ğŸ’¡ ä½¿ç”¨æ¼”ç¤ºæ•°æ®ï¼ˆæ¼”ç¤ºæ°¸ä¸ç¿»è½¦ï¼‰")
    else:
        data_path = str(project_root / "data/clean/annotations_clean.jsonl")
        st.caption("ğŸ’¡ ä½¿ç”¨çœŸå®çˆ¬å–æ•°æ®")
        
        # === çˆ¬å–è®¾ç½®ï¼ˆä»…çœŸå®æ•°æ®æ¨¡å¼æ˜¾ç¤ºï¼‰===
        with st.expander("ğŸš€ çˆ¬å–æ–°æ•°æ®", expanded=False):
            crawl_keyword = st.text_input(
                "å…³é”®è¯",
                value="AIå·¥å…·",
                placeholder="è¾“å…¥æœç´¢å…³é”®è¯",
                help="å°çº¢ä¹¦æœç´¢å…³é”®è¯"
            )
            
            crawl_count = st.slider(
                "çˆ¬å–æ•°é‡",
                min_value=5,
                max_value=30,
                value=10,
                step=5,
                help="å»ºè®® 10-20 æ¡ï¼Œé¿å…è§¦å‘åçˆ¬"
            )
            
            if st.button("ğŸ•·ï¸ å¼€å§‹çˆ¬å–", use_container_width=True):
                st.session_state.trigger_crawl = True
                st.session_state.crawl_keyword = crawl_keyword
                st.session_state.crawl_count = crawl_count
    
    # === ä¸€é”®æŒ–æ˜æŒ‰é’® ===
    st.markdown("---")
    mine_button = st.button("ğŸ” Mineï¼ˆæŒ–æ˜ï¼‰", type="primary", use_container_width=True)
    
    if mine_button:
        st.session_state.mining_done = True
        st.session_state.trigger_mine = True
    
    # === Demo Mode æ ·ä¾‹åº“ç®¡ç† ===
    st.markdown("---")
    st.subheader("ğŸ“¦ æ ·ä¾‹æ•°æ®ç®¡ç†")
    
    sample_path = str(project_root / "data/samples/annotations_sample.jsonl")
    raw_path = str(project_root / "data/raw/annotations.jsonl")
    
    demo_action = st.radio(
        "æ“ä½œæ¨¡å¼",
        ["ä»…æŸ¥çœ‹", "è¦†ç›–åˆ° raw", "åˆå¹¶å»é‡åˆ° raw"],
        help="æ ·ä¾‹æ•°æ®æ“ä½œï¼šæŸ¥çœ‹ã€è¦†ç›–æˆ–åˆå¹¶"
    )
    
    if st.button("æ‰§è¡Œæ ·ä¾‹æ“ä½œ", use_container_width=True):
        try:
            if demo_action == "è¦†ç›–åˆ° raw":
                shutil.copy(sample_path, raw_path)
                count = count_lines(raw_path)
                st.success(f"âœ… å·²è¦†ç›–åˆ° raw ({count}æ¡)")
                st.rerun()
            
            elif demo_action == "åˆå¹¶å»é‡åˆ° raw":
                merged_count = merge_jsonl_files([raw_path, sample_path], raw_path)
                st.success(f"âœ… å·²åˆå¹¶å»é‡ ({merged_count}æ¡)")
                st.rerun()
            
            else:
                sample_items = load_jsonl(sample_path)
                st.info(f"ğŸ“‹ æ ·ä¾‹æ•°æ®: {len(sample_items)} æ¡")
        
        except Exception as e:
            st.error(f"æ“ä½œå¤±è´¥: {e}")
    
    # === æ•°æ®ç»Ÿè®¡ ===
    st.markdown("---")
    st.subheader("ğŸ“Š æ•°æ®ç»Ÿè®¡")
    
    raw_count = count_lines(str(project_root / "data/raw/annotations.jsonl"))
    clean_count = count_lines(str(project_root / "data/clean/annotations_clean.jsonl"))
    current_count = count_lines(data_path)
    
    st.metric("Raw è¡Œæ•°", raw_count)
    st.metric("Clean è¡Œæ•°", clean_count)
    st.metric("å½“å‰ä½¿ç”¨", current_count)
    
    with st.expander("ğŸ“ æ–‡ä»¶è·¯å¾„"):
        st.code(f"Raw: {str(project_root / 'data/raw/')}", language="text")
        st.code(f"Clean: {str(project_root / 'data/clean/')}", language="text")
    
    # === å¯¼å‡ºæäº¤åŒ… ===
    st.markdown("---")
    st.subheader("ğŸ“¦ å¯¼å‡ºæäº¤åŒ…")
    
    if st.button("ğŸ ç”Ÿæˆ Submission ZIP", use_container_width=True):
        with st.spinner("æ­£åœ¨æ‰“åŒ…..."):
            try:
                zip_path, stats = create_submission_package(
                    output_dir=str(project_root / "data/exports"),
                    project_root=str(project_root)
                )
                
                # æä¾›ä¸‹è½½
                with open(zip_path, "rb") as f:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½æäº¤åŒ…",
                        data=f.read(),
                        file_name=f"submission_{timestamp}.zip",
                        mime="application/zip",
                        use_container_width=True
                    )
                
                st.success(f"âœ… åŒ…å« {len(stats['files_included'])} ä¸ªæ–‡ä»¶ ({stats['total_size_mb']:.2f} MB)")
            
            except Exception as e:
                st.error(f"æ‰“åŒ…å¤±è´¥: {e}")


# ============= ä¸»åŒºåŸŸ =============

# åˆå§‹åŒ– session state
if 'logs' not in st.session_state:
    st.session_state.logs = []

def add_log(msg, level="INFO"):
    """æ·»åŠ æ—¥å¿—"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    log_entry = f"[{timestamp}] {level}: {msg}"
    st.session_state.logs.append(log_entry)
    if len(st.session_state.logs) > 50:  # ä¿ç•™æœ€è¿‘50æ¡
        st.session_state.logs = st.session_state.logs[-50:]


def fix_html_relative_paths(html_content: str, html_path: str) -> str:
    """
    ä¿®å¤ HTML ä¸­çš„ç›¸å¯¹è·¯å¾„ï¼Œå°†æœ¬åœ° JS æ–‡ä»¶å†…åµŒåˆ° HTML ä¸­
    
    Args:
        html_content: HTML å†…å®¹
        html_path: HTML æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºè§£æç›¸å¯¹è·¯å¾„ï¼‰
    
    Returns:
        ä¿®å¤åçš„ HTML å†…å®¹
    """
    import re
    from pathlib import Path
    
    # è·å– HTML æ–‡ä»¶æ‰€åœ¨ç›®å½•
    html_dir = Path(html_path).parent
    project_root = Path(__file__).parent.parent.parent
    
    # æŸ¥æ‰¾æ‰€æœ‰ç›¸å¯¹è·¯å¾„çš„ script æ ‡ç­¾
    pattern = r'<script\s+src=["\']([^"\']+)["\']\s*></script>'
    
    def replace_script(match):
        script_path = match.group(1)
        
        # åªå¤„ç†ç›¸å¯¹è·¯å¾„ï¼ˆä¸ä»¥ http:// æˆ– https:// å¼€å¤´ï¼‰
        if script_path.startswith(('http://', 'https://', '//')):
            return match.group(0)  # ä¿æŒ CDN é“¾æ¥ä¸å˜
        
        # è§£æç›¸å¯¹è·¯å¾„
        if script_path.startswith('/'):
            # ç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
            full_path = project_root / script_path.lstrip('/')
        else:
            # ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äº HTML æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼‰
            full_path = html_dir / script_path
        
        # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¯»å–å¹¶å†…åµŒ
        if full_path.exists() and full_path.is_file():
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    js_content = f.read()
                # æ›¿æ¢ä¸ºå†…åµŒ script
                return f'<script>\n{js_content}\n</script>'
            except Exception as e:
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œä¿æŒåŸæ ·
                return match.group(0)
        else:
            # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¿æŒåŸæ ·
            return match.group(0)
    
    # æ›¿æ¢æ‰€æœ‰åŒ¹é…çš„ script æ ‡ç­¾
    fixed_html = re.sub(pattern, replace_script, html_content)
    
    return fixed_html


# === çˆ¬å–æµç¨‹ï¼ˆçœŸå®æ•°æ®æ¨¡å¼ï¼‰===

if st.session_state.get("trigger_crawl", False):
    st.session_state.trigger_crawl = False
    
    keyword = st.session_state.get("crawl_keyword", "AIå·¥å…·")
    count = st.session_state.get("crawl_count", 10)
    
    st.info(f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å–å…³é”®è¯ã€Œ{keyword}ã€ï¼Œç›®æ ‡ {count} æ¡...")
    st.warning("âš ï¸ æµè§ˆå™¨å°†åœ¨æ–°çª—å£æ‰“å¼€ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼ˆå¦‚éœ€è¦ï¼‰")
    
    progress_bar = st.progress(0, text="å¯åŠ¨çˆ¬è™«å­è¿›ç¨‹...")
    status_text = st.empty()
    
    try:
        import subprocess
        import sys
        import locale
        
        # ä½¿ç”¨å­è¿›ç¨‹è¿è¡Œçˆ¬è™«è„šæœ¬ï¼ˆé¿å… Streamlit ç¯å¢ƒçš„ asyncio å†²çªï¼‰
        crawl_script = str(project_root / "scripts" / "test_crawl_raw.py")
        
        status_text.text("ğŸ”„ å¯åŠ¨çˆ¬è™«ï¼ˆæ–°çª—å£ï¼‰...")
        progress_bar.progress(20)
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            sys.executable,
            crawl_script,
            "--keyword", keyword,
            "--count", str(count)
        ]
        
        # Windows ç¼–ç ä¿®å¤
        encoding = 'utf-8' if sys.platform != 'win32' else locale.getpreferredencoding(False)
        
        # è¿è¡Œå­è¿›ç¨‹
        result = subprocess.run(
            cmd,
            cwd=str(project_root),
            capture_output=True,
            text=True,
            encoding=encoding,
            errors='replace',  # é‡åˆ°æ— æ³•è§£ç çš„å­—ç¬¦ç”¨æ›¿ä»£ç¬¦å·
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        progress_bar.progress(70)
        
        if result.returncode == 0:
            status_text.text("ğŸ”„ æ¸…æ´—æ•°æ®...")
            
            # è¿è¡Œæ¸…æ´—
            from src.pipeline.cleaner import DataCleaner
            cleaner = DataCleaner()
            clean_count = cleaner.clean()
            
            progress_bar.progress(100)
            progress_bar.empty()
            status_text.empty()
            
            st.success(f"âœ… çˆ¬å–å®Œæˆï¼æ¸…æ´—å {clean_count} æ¡")
            st.info("ğŸ’¡ ç°åœ¨å¯ä»¥ç‚¹å‡» **Mine** æŒ‰é’®åˆ†ææ–°æ•°æ®")
            
            # æ˜¾ç¤ºçˆ¬è™«è¾“å‡ºï¼ˆæ·»åŠ ç©ºå€¼æ£€æŸ¥ï¼‰
            with st.expander("ğŸ“‹ çˆ¬è™«æ—¥å¿—", expanded=False):
                stdout = result.stdout or ""
                st.code(stdout[-2000:] if len(stdout) > 2000 else stdout)
            
            # æ¸…é™¤æ—§æ•°æ®ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½
            st.session_state.mining_done = False
            st.session_state.mine_done = False
            st.session_state.items = []  # æ¸…é™¤ç¼“å­˜çš„æ•°æ®
            st.session_state.pagerank_top = []
            st.session_state.rising_edges = []
            st.session_state.graph_obj = None
            st.session_state.graph_path = None
            
            # å¼ºåˆ¶åˆ·æ–° Dashboard ä»¥æ˜¾ç¤ºæ–°æ•°æ®
            st.rerun()
            
        else:
            progress_bar.empty()
            status_text.empty()
            st.error(f"âŒ çˆ¬å–å¤±è´¥ï¼ˆé€€å‡ºç : {result.returncode}ï¼‰")
            with st.expander("ğŸ“‹ é”™è¯¯è¯¦æƒ…", expanded=True):
                stderr = result.stderr or ""
                stdout = result.stdout or ""
                st.code(stderr[-2000:] if stderr else stdout[-2000:])
        
    except subprocess.TimeoutExpired:
        progress_bar.empty()
        status_text.empty()
        st.error("âŒ çˆ¬å–è¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰")
        st.info("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œæˆ–å‡å°‘çˆ¬å–æ•°é‡")
        
    except Exception as e:
        progress_bar.empty()
        status_text.empty()
        st.error(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        st.info("ğŸ’¡ å¯èƒ½åŸå› ï¼šç½‘ç»œé—®é¢˜ã€ç™»å½•è¿‡æœŸã€åçˆ¬é™åˆ¶")


# === æŒ–æ˜æµç¨‹ ===

if not st.session_state.get("mining_done", False):
    # æœªå¼€å§‹æŒ–æ˜
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§é€‰æ‹©æ•°æ®æºï¼Œç„¶åç‚¹å‡» **Mine** æŒ‰é’®å¼€å§‹æŒ–æ˜")
    
    # æ˜¾ç¤ºè¯´æ˜
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“– åŠŸèƒ½è¯´æ˜")
        st.markdown("""
        - **å›¾è°±åˆ†æ**: æ ‡ç­¾å…±ç°ç½‘ç»œå¯è§†åŒ–
        - **PageRank Top**: æ ¸å¿ƒè¯é¢˜æ ‡ç­¾æ’å
        - **Rising Edges**: è¶‹åŠ¿ç»„åˆå‘ç°
        - **åŸå¸–æ ·æœ¬**: æ•°æ®æºå†…å®¹å±•ç¤º
        - **Demo Mode**: æ¼”ç¤ºå…œåº•ï¼ˆæ°¸ä¸ç¿»è½¦ï¼‰
        - **ä¸€é”®å¯¼å‡º**: æäº¤åŒ…ç”Ÿæˆ
        """)
    
    with col2:
        st.subheader("ğŸ“ ä½¿ç”¨å»ºè®®")
        st.markdown("""
        1. é¦–æ¬¡ä½¿ç”¨å»ºè®®é€‰æ‹© **Sample Data**
        2. ç‚¹å‡» **Mine** åç­‰å¾… 10-30 ç§’
        3. å¯ä¸‹è½½å›¾è°± HTML æœ¬åœ°æŸ¥çœ‹
        4. åˆ‡æ¢æ•°æ®æºåéœ€é‡æ–°æŒ–æ˜
        5. æ¼”ç¤ºå‰å»ºè®®å…ˆæµ‹è¯•ä¸€éå®Œæ•´æµç¨‹
        """)

elif st.session_state.get("trigger_mine", False):
    # å¼€å§‹æŒ–æ˜
    st.session_state.trigger_mine = False
    
    # è¿›åº¦æ˜¾ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # æ—¥å¿—æ˜¾ç¤º
    log_expander = st.expander("ğŸ“‹ è¯¦ç»†æ—¥å¿—", expanded=True)
    with log_expander:
        log_area = st.empty()
    
    try:
        # === æ­¥éª¤ 1: åŠ è½½æ•°æ® ===
        status_text.text("ğŸ”„ æ­¥éª¤ 1/5: åŠ è½½æ•°æ®...")
        progress_bar.progress(0.1)
        add_log("å¼€å§‹åŠ è½½æ•°æ®æ–‡ä»¶")
        
        builder = TagCooccurrenceGraph(data_path)
        items = builder.load_data()
        
        if not items:
            add_log("âŒ æ•°æ®æ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨", "ERROR")
            st.error("æ•°æ®æ–‡ä»¶ä¸ºç©ºï¼Œè¯·å…ˆçˆ¬å–æ•°æ®æˆ–ä½¿ç”¨ Sample Data")
            st.session_state.mining_done = False
            st.stop()
        
        add_log(f"âœ… æˆåŠŸåŠ è½½ {len(items)} æ¡æ•°æ®", "SUCCESS")
        log_area.code("\n".join(st.session_state.logs[-10:]))
        
        # === æ­¥éª¤ 2: æ„å»ºå›¾è°± ===
        status_text.text("ğŸ”„ æ­¥éª¤ 2/5: æ„å»ºå›¾è°±...")
        progress_bar.progress(0.3)
        add_log("å¼€å§‹æ„å»ºæ ‡ç­¾å…±ç°å›¾")
        log_area.code("\n".join(st.session_state.logs[-10:]))
        
        graph = builder.build_graph()
        
        if graph.number_of_nodes() == 0:
            add_log("âŒ å›¾è°±ä¸ºç©ºï¼ˆæ ‡ç­¾æ•°é‡ä¸è¶³ï¼‰", "ERROR")
            st.error("æ ‡ç­¾æ•°é‡ä¸è¶³ï¼Œæ— æ³•æ„å»ºå›¾è°±")
            st.session_state.mining_done = False
            st.stop()
        
        add_log(f"âœ… å›¾è°±æ„å»ºå®Œæˆ: {graph.number_of_nodes()} èŠ‚ç‚¹, {graph.number_of_edges()} è¾¹", "SUCCESS")
        log_area.code("\n".join(st.session_state.logs[-10:]))
        
        # === æ­¥éª¤ 3: è®¡ç®— PageRank ===
        status_text.text("ğŸ”„ æ­¥éª¤ 3/5: è®¡ç®— PageRank...")
        progress_bar.progress(0.5)
        add_log("å¼€å§‹ PageRank è®¡ç®—")
        log_area.code("\n".join(st.session_state.logs[-10:]))
        
        analytics = GraphAnalytics(graph, data_path)
        pagerank_top = analytics.compute_pagerank(top_n=15)
        
        add_log(f"âœ… PageRank å®Œæˆ: Top {len(pagerank_top)} æ ‡ç­¾", "SUCCESS")
        log_area.code("\n".join(st.session_state.logs[-10:]))
        
        # === æ­¥éª¤ 4: å‘ç°è¶‹åŠ¿è¾¹ ===
        status_text.text("ğŸ”„ æ­¥éª¤ 4/5: å‘ç°è¶‹åŠ¿è¾¹...")
        progress_bar.progress(0.7)
        add_log("å¼€å§‹ Rising Edges åˆ†æ")
        log_area.code("\n".join(st.session_state.logs[-10:]))
        
        rising_edges, window_stats = analytics.find_rising_edges(
            recent_days=7,
            historical_days=30,
            top_n=10
        )
        
        add_log(f"âœ… è¶‹åŠ¿åˆ†æå®Œæˆ: Recent {window_stats['recent_count']} | Historical {window_stats['historical_count']}", "SUCCESS")
        log_area.code("\n".join(st.session_state.logs[-10:]))
        
        # === æ­¥éª¤ 5: ç”Ÿæˆå¯è§†åŒ– ===
        status_text.text("ğŸ”„ æ­¥éª¤ 5/5: ç”Ÿæˆå¯è§†åŒ–...")
        progress_bar.progress(0.9)
        add_log("å¼€å§‹ç”Ÿæˆäº¤äº’å¼å›¾è°±")
        log_area.code("\n".join(st.session_state.logs[-10:]))
        
        visualizer = GraphVisualizer(graph, dict(pagerank_top))
        graph_path = str(project_root / "data/output/graph.html")
        visualizer.create_interactive_html(graph_path)
        
        add_log(f"âœ… å›¾è°±å·²ç”Ÿæˆ: {graph_path}", "SUCCESS")
        log_area.code("\n".join(st.session_state.logs[-10:]))
        
        # === å®Œæˆ ===
        progress_bar.progress(1.0)
        status_text.text("âœ… æŒ–æ˜å®Œæˆï¼")
        add_log("ğŸ‰ æ‰€æœ‰æ­¥éª¤æˆåŠŸå®Œæˆ", "SUCCESS")
        log_area.code("\n".join(st.session_state.logs[-10:]))
        
        # ä¿å­˜ç»“æœåˆ° session state
        st.session_state.pagerank_top = pagerank_top
        st.session_state.rising_edges = rising_edges
        st.session_state.window_stats = window_stats
        st.session_state.graph_path = graph_path
        st.session_state.items = items
        st.session_state.graph_nodes = graph.number_of_nodes()
        st.session_state.graph_edges = graph.number_of_edges()
        st.session_state.graph_obj = graph  # ä¿å­˜å›¾å¯¹è±¡ï¼ˆç”¨äºæ´å¯Ÿé¢æ¿ï¼‰
        st.session_state.mine_done = True  # æ ‡è®°æŒ–æ˜å®Œæˆ
        
        # æ¸…é™¤è§¦å‘çŠ¶æ€ï¼Œå‡†å¤‡æ˜¾ç¤ºç»“æœ
        st.session_state.trigger_mine = False
        st.session_state.show_results = True  # æ ‡è®°æ˜¾ç¤ºç»“æœ
        
        st.success("âœ… æŒ–æ˜å®Œæˆï¼è¯·æŸ¥çœ‹ä¸‹æ–¹ç»“æœ")
        
        # å¼ºåˆ¶åˆ·æ–°ä»¥æ˜¾ç¤ºç»“æœ
        st.rerun()
        
    except Exception as e:
        add_log(f"âŒ å¤±è´¥: {str(e)}", "ERROR")
        log_area.code("\n".join(st.session_state.logs[-10:]))
        st.error(f"æŒ–æ˜å¤±è´¥: {e}")
        st.session_state.mining_done = False
        st.session_state.trigger_mine = False
        progress_bar.progress(0)
        status_text.text("âŒ æŒ–æ˜å¤±è´¥")
        st.stop()

if st.session_state.get("show_results", False):
    # æ˜¾ç¤ºç»“æœï¼ˆMine æˆåŠŸåï¼‰
    st.success("âœ… æŒ–æ˜å®Œæˆï¼")
    
    # Tabsï¼ˆæ–°å¢ Generate Tabï¼‰
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š å›¾è°±åˆ†æ", "ğŸ“ åŸå¸–æ ·æœ¬", "âœ¨ ç”Ÿæˆæ–‡æ¡ˆ"])
    
    with tab1:
        # === æ´å¯Ÿä¸å»ºè®®é¢æ¿ï¼ˆæ–°å¢ï¼Œæ”¾åœ¨æœ€ä¸Šæ–¹ï¼‰===
        # ä» session_state è·å–æ•°æ®ï¼ˆå…œåº•æœºåˆ¶ï¼‰
        graph_obj = st.session_state.get("graph_obj")
        pr_top = st.session_state.get("pagerank_top", [])
        rs_edges = st.session_state.get("rising_edges", [])
        ws_stats = st.session_state.get("window_stats", {})
        
        if graph_obj and pr_top:
            try:
                render_insights_panel(
                    graph=graph_obj,
                    pagerank_top=pr_top,
                    rising_edges=rs_edges,
                    window_stats=ws_stats,
                    keyword="AIå·¥å…·"
                )
                st.markdown("---")
            except Exception as e:
                st.warning(f"æ´å¯Ÿé¢æ¿åŠ è½½å¤±è´¥: {e}")
        elif not pr_top:
            st.info("ğŸ’¡ è¯·å…ˆç‚¹å‡»ä¾§è¾¹æ çš„ **Mine** æŒ‰é’®ç”Ÿæˆå›¾è°±åˆ†æç»“æœ")
        
        # === å›¾è°±å¯è§†åŒ– ===
        st.subheader("ğŸ•¸ï¸ æ ‡ç­¾å…±ç°å›¾è°±")
        
        graph_path = st.session_state.get("graph_path")
        
        if not graph_path or not os.path.exists(graph_path):
            st.warning("âš ï¸ å›¾è°±æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆç‚¹å‡» Mine æŒ‰é’®")
        else:
            try:
                # è¯»å– HTML å†…å®¹
                with open(graph_path, "r", encoding="utf-8") as f:
                    html_content = f.read()
                
                if not html_content or len(html_content) < 100:
                    st.error(f"âŒ å›¾è°±æ–‡ä»¶ä¸ºç©ºæˆ–æŸå: {graph_path}")
                else:
                    # ä¿®å¤ç›¸å¯¹è·¯å¾„é—®é¢˜ï¼šå†…åµŒæœ¬åœ° JS æ–‡ä»¶
                    html_content = fix_html_relative_paths(html_content, graph_path)
                    
                    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
                    st.caption(f"ğŸ“ å›¾è°±æ–‡ä»¶: {graph_path}")
                    
                    # å†…åµŒå›¾è°±ï¼ˆå…³é”®ï¼šè¶³å¤Ÿçš„é«˜åº¦ + å…è®¸æ»šåŠ¨ï¼‰
                    st.components.v1.html(html_content, height=800, scrolling=True)
                    
                    # ä¸‹è½½å’ŒæŸ¥çœ‹æŒ‰é’®
                    col1, col2 = st.columns([1, 3])
                    with col1:
                        with open(graph_path, "rb") as f:
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½å›¾è°± HTML",
                                data=f.read(),
                                file_name="tag_graph.html",
                                mime="text/html",
                                use_container_width=True
                            )
                    with col2:
                        st.info(f"ğŸ’¡ å›¾è°±å·²ç”Ÿæˆï¼ŒåŒ…å« {st.session_state.get('graph_nodes', 0)} ä¸ªèŠ‚ç‚¹")
            
            except Exception as e:
                st.error(f"âŒ å›¾è°±åŠ è½½å¤±è´¥: {e}")
                st.code(f"è·¯å¾„: {graph_path}")
                import traceback
                st.code(traceback.format_exc())
        
        st.markdown("---")
        
        # === ä¸¤åˆ—ï¼šPageRank + Rising Edges ===
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ† PageRank Top æ¦œå•")
            st.caption("æ ¸å¿ƒè¯é¢˜æ ‡ç­¾æ’åï¼ˆåŸºäºå›¾ç»“æ„é‡è¦æ€§ï¼‰")
            
            pagerank_top = st.session_state.get("pagerank_top", [])
            if pagerank_top:
                import pandas as pd
                df = pd.DataFrame({
                    "æ’å": list(range(1, len(pagerank_top) + 1)),
                    "æ ‡ç­¾": [tag for tag, _ in pagerank_top],
                    "PageRank": [f"{score:.4f}" for _, score in pagerank_top]
                })
                st.dataframe(df, use_container_width=True, hide_index=True)
            else:
                st.info("æš‚æ— æ•°æ®")
        
        with col2:
            st.subheader("ğŸ”¥ Rising Edges è¶‹åŠ¿æ¦œ")
            
            window_stats = st.session_state.get("window_stats", {})
            rising_edges = st.session_state.get("rising_edges", [])
            
            # æ˜¾ç¤ºè¯Šæ–­ä¿¡æ¯
            mode = window_stats.get('mode', 'rising')
            anchor_now = window_stats.get('anchor_now', 'N/A')
            
            # æ¨¡å¼æ ‡è¯†
            if mode == "fallback":
                st.warning("âš ï¸ æ¨¡å¼: Fallback (çª—å£æ ·æœ¬ä¸è¶³ï¼Œæ˜¾ç¤ºå…¨å±€ Top Edges)")
            else:
                st.success("âœ… æ¨¡å¼: Rising (åŸºäºæ—¶é—´çª—å£å¯¹æ¯”)")
            
            # çª—å£ç»Ÿè®¡
            st.caption(
                f"Anchor: {anchor_now} | "
                f"Recent: {window_stats.get('recent_count', 0)} | "
                f"Historical: {window_stats.get('historical_count', 0)} | "
                f"Total: {window_stats.get('total_count', 0)}"
            )
            
            if rising_edges:
                import pandas as pd
                
                # æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒåˆ—
                if mode == "fallback":
                    df = pd.DataFrame({
                        "æ’å": list(range(1, len(rising_edges) + 1)),
                        "æ ‡ç­¾ç»„åˆ": [f"{tag1} â†” {tag2}" for tag1, tag2, _, _ in rising_edges],
                        "å…±ç°æ¬¡æ•°": [details.get('total_count', 0) for _, _, _, details in rising_edges]
                    })
                else:
                    df = pd.DataFrame({
                        "æ’å": list(range(1, len(rising_edges) + 1)),
                        "æ ‡ç­¾ç»„åˆ": [f"{tag1} â†” {tag2}" for tag1, tag2, _, _ in rising_edges],
                        "å¢å¹…": [f"+{details.get('growth_rate', 0)*100:.1f}%" for _, _, _, details in rising_edges],
                        "Recent": [details.get('recent_count', 0) for _, _, _, details in rising_edges],
                        "Historical": [details.get('historical_count', 0) for _, _, _, details in rising_edges]
                    })
                
                st.dataframe(df, use_container_width=True, hide_index=True)
            else:
                st.info("æš‚æ— æ•°æ®")
    
    with tab2:
        st.subheader("ğŸ“„ åŸå¸–æ ·æœ¬")
        
        items = st.session_state.get("items", [])
        
        if items:
            # åˆ†é¡µæ˜¾ç¤º
            items_per_page = 10
            total_pages = (len(items) + items_per_page - 1) // items_per_page
            
            page = st.selectbox("é¡µç ", range(1, total_pages + 1))
            start_idx = (page - 1) * items_per_page
            end_idx = min(start_idx + items_per_page, len(items))
            
            page_items = items[start_idx:end_idx]
            
            for i, item in enumerate(page_items, start_idx + 1):
                with st.expander(f"ğŸ“ ç¬”è®° {i}: {item.get('title', 'æ— æ ‡é¢˜')[:60]}..."):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        st.markdown(f"**æ ‡é¢˜**: {item.get('title', '')}")
                        desc = item.get('desc', '')
                        st.markdown(f"**æè¿°**: {desc[:300]}{'...' if len(desc) > 300 else ''}")
                        st.markdown(f"**æ—¶é—´**: {item.get('time', 'æœªçŸ¥')}")
                    
                    with col2:
                        tags = item.get('tags', [])
                        st.markdown(f"**æ ‡ç­¾** ({len(tags)}):")
                        st.write(", ".join(tags[:6]))
                        if len(tags) > 6:
                            st.caption(f"...è¿˜æœ‰ {len(tags)-6} ä¸ª")
                        
                        st.markdown(f"**å›¾ç‰‡æ•°**: {len(item.get('images', []))}")
                        
                        if item.get('url'):
                            st.markdown(f"[ğŸ”— æŸ¥çœ‹åŸæ–‡]({item['url']})")
        else:
            st.info("æš‚æ— æ•°æ®")
    
    with tab3:
        st.subheader("âœ¨ æ–‡æ¡ˆ/ç´ æåŒ…ç”Ÿæˆ")
        st.caption("åŸºäºæŒ–æ˜ç»“æœç”Ÿæˆåˆ›ä½œç´ æï¼ˆæ¨¡æ¿å¼•æ“ï¼Œæ— éœ€ LLMï¼‰")
        
        # æ£€æŸ¥æ˜¯å¦å·²æŒ–æ˜
        pagerank_top = st.session_state.get("pagerank_top", [])
        rising_edges = st.session_state.get("rising_edges", [])
        
        if not pagerank_top:
            st.warning("âš ï¸ è¯·å…ˆåœ¨å·¦ä¾§ç‚¹å‡» Mine å®ŒæˆæŒ–æ˜")
        else:
            # ç”Ÿæˆå‚æ•°
            col1, col2 = st.columns(2)
            
            with col1:
                gen_keyword = st.text_input(
                    "å…³é”®è¯",
                    value="AIå·¥å…·",
                    help="æ–‡æ¡ˆä¸»é¢˜å…³é”®è¯"
                )
                
                gen_count = st.number_input(
                    "ç”Ÿæˆæ•°é‡",
                    min_value=1,
                    max_value=20,
                    value=5,
                    help="ç”Ÿæˆè‰ç¨¿æ•°é‡"
                )
            
            with col2:
                account_mode = st.radio(
                    "è´¦å·æ¨¡å¼",
                    ["å•è´¦å·", "å¤šè´¦å·ï¼ˆ3ä¸ªï¼‰"],
                    help="åˆ†é…åˆ°ä¸åŒè´¦å·"
                )
                
                image_mode = st.radio(
                    "å›¾ç‰‡æ¨¡å¼",
                    ["No images", "Source images (å¼•ç”¨åŸå¸–)"],
                    help="ç´ æåŒ…æ˜¯å¦åŒ…å«å›¾ç‰‡"
                )
            
            # LLM å¯é€‰å¢å¼º
            st.markdown("---")
            use_llm = st.checkbox(
                "ğŸ¤– Use LLM Enhanceï¼ˆå¯é€‰ï¼‰",
                value=False,
                help="ä½¿ç”¨å¤§æ¨¡å‹ä¼˜åŒ–æ–‡æ¡ˆï¼ˆéœ€é…ç½® API Keyï¼‰"
            )
            
            if use_llm:
                with st.expander("âš™ï¸ LLM é…ç½®", expanded=False):
                    llm_provider = st.selectbox(
                        "Provider",
                        ["DeepSeek", "OpenAI", "é€šä¹‰åƒé—®", "æ–‡å¿ƒä¸€è¨€"],
                        help="é€‰æ‹©å¤§æ¨¡å‹æä¾›å•†"
                    )
                    
                    llm_api_key = st.text_input(
                        "API Key",
                        type="password",
                        placeholder="sk-...",
                        help="ç•™ç©ºåˆ™ä½¿ç”¨æ¨¡æ¿å¼•æ“"
                    )
                    
                    if not llm_api_key:
                        st.warning("âš ï¸ æœªé…ç½® API Keyï¼Œå°†ä½¿ç”¨æ¨¡æ¿å¼•æ“ç”Ÿæˆ")
            
            st.markdown("---")
            
            # æ£€æŸ¥æ¨¡æ¿å¼•æ“æ˜¯å¦å¯ç”¨
            if not TEMPLATE_ENGINE_AVAILABLE:
                st.error(f"âŒ ç”Ÿæˆæ¨¡å—æœªå°±ç»ªï¼š{TEMPLATE_ENGINE_ERROR}")
                st.info("ğŸ’¡ æç¤ºï¼šå›¾è°±åˆ†æå’ŒåŸå¸–æ ·æœ¬åŠŸèƒ½ä»å¯æ­£å¸¸ä½¿ç”¨")
                st.stop()
            
            # ç”ŸæˆæŒ‰é’®
            if st.button("ğŸ¨ ç”Ÿæˆæ–‡æ¡ˆåŒ…", type="primary", use_container_width=True):
                with st.spinner("æ­£åœ¨ç”Ÿæˆæ–‡æ¡ˆ..."):
                    try:
                        # å‡†å¤‡æ•°æ®
                        top_tags = [tag for tag, _ in pagerank_top[:10]]
                        top_edges_data = [(t1, t2, 0.0) for t1, t2, _, _ in rising_edges[:10]]
                        
                        # åˆ›å»ºç”Ÿæˆå™¨
                        engine = TemplateEngine(top_tags, top_edges_data)
                        
                        # æ£€æŸ¥ LLM é…ç½®
                        use_llm_generation = False
                        if use_llm:
                            if llm_api_key and llm_api_key.strip():
                                use_llm_generation = True
                                st.info(f"ğŸ¤– ä½¿ç”¨ {llm_provider} ç”Ÿæˆ")
                            else:
                                st.warning("âš ï¸ API Key æœªé…ç½®ï¼Œä½¿ç”¨æ¨¡æ¿å¼•æ“")
                        
                        # ç”Ÿæˆè‰ç¨¿
                        if account_mode == "å¤šè´¦å·ï¼ˆ3ä¸ªï¼‰":
                            accounts = ["æµ‹è¯„å·", "æ•™ç¨‹å·", "æ•ˆç‡å·"]
                        else:
                            accounts = ["ä¸»è´¦å·"]
                        
                        drafts = []
                        llm_success_count = 0
                        
                        # å¦‚æœå¯ç”¨ LLM ä¸”æœ‰ API Key
                        if use_llm_generation and LLM_CLIENT_AVAILABLE and generate_with_llm:
                            styles = ["æ¸…å•å‹", "å¯¹æ¯”å‹", "é¿å‘å‹", "æ•™ç¨‹å‹"]
                            
                            # è·å–åŸå¸–æ ‡é¢˜ä½œä¸ºå‚è€ƒ
                            original_titles = []
                            items = st.session_state.get("items", [])
                            for item in items[:5]:
                                if item.get("title"):
                                    original_titles.append(item["title"])
                            
                            # å°è¯•ç”¨ LLM ç”Ÿæˆ
                            progress_bar = st.progress(0, text="æ­£åœ¨è°ƒç”¨ LLM API...")
                            
                            for i in range(gen_count):
                                style = styles[i % len(styles)]
                                progress_bar.progress((i + 1) / gen_count, text=f"LLM ç”Ÿæˆä¸­... {i+1}/{gen_count}")
                                
                                try:
                                    llm_result = generate_with_llm(
                                        keyword=gen_keyword,
                                        top_tags=top_tags,
                                        top_edges=[(t1, t2) for t1, t2, _ in top_edges_data],
                                        provider=llm_provider,
                                        api_key=llm_api_key,
                                        style=style,
                                        original_titles=original_titles
                                    )
                                except Exception as llm_err:
                                    st.warning(f"âš ï¸ LLM è°ƒç”¨å¼‚å¸¸: {llm_err}")
                                    llm_result = None
                                
                                if llm_result:
                                    llm_result["account"] = accounts[i % len(accounts)]
                                    llm_result["content_style"] = style
                                    drafts.append(llm_result)
                                    llm_success_count += 1
                                else:
                                    # LLM å¤±è´¥ï¼Œç”¨æ¨¡æ¿å¼•æ“è¡¥å……
                                    template_draft = engine.generate_draft(gen_keyword)
                                    template_draft["account"] = accounts[i % len(accounts)]
                                    template_draft["fallback_reason"] = "LLM API è°ƒç”¨å¤±è´¥"
                                    drafts.append(template_draft)
                            
                            progress_bar.empty()
                            
                            if llm_success_count > 0:
                                st.success(f"ğŸ¤– LLM æˆåŠŸç”Ÿæˆ {llm_success_count} æ¡")
                            if llm_success_count < gen_count:
                                st.warning(f"âš ï¸ {gen_count - llm_success_count} æ¡ä½¿ç”¨æ¨¡æ¿å¼•æ“å›é€€")
                        
                        else:
                            # ä½¿ç”¨æ¨¡æ¿å¼•æ“ç”Ÿæˆ
                            drafts = engine.generate_batch(
                                keyword=gen_keyword,
                                count=gen_count,
                                accounts=accounts
                            )
                        
                        # ä¿å­˜åˆ° session state
                        st.session_state.generated_drafts = drafts
                        st.session_state.package_keyword = gen_keyword
                        
                        st.success(f"âœ… å·²ç”Ÿæˆ {len(drafts)} æ¡è‰ç¨¿")
                        st.rerun()
                    
                    except Exception as e:
                        st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
            
            # æ˜¾ç¤ºç”Ÿæˆç»“æœ
            if st.session_state.get("generated_drafts"):
                drafts = st.session_state.generated_drafts
                
                st.markdown("---")
                st.subheader(f"ğŸ“ è‰ç¨¿é¢„è§ˆï¼ˆå…± {len(drafts)} æ¡ï¼‰")
                
                # æ˜¾ç¤ºå‰3æ¡é¢„è§ˆ
                for i, draft in enumerate(drafts[:3], 1):
                    with st.expander(f"è‰ç¨¿ {i}/{len(drafts)}: {draft['title'][:50]}..."):
                        st.markdown(f"**è´¦å·**: {draft.get('account', 'N/A')}")
                        st.markdown(f"**æ ‡é¢˜**: {draft['title']}")
                        st.markdown(f"**æ­£æ–‡**:\n\n{draft['body']}")
                        st.markdown(f"**æ ‡ç­¾**: {', '.join(draft['hashtags'][:6])}")
                        st.markdown(f"**ç”Ÿæˆæ–¹å¼**: {draft.get('generation_method', 'template')}")
                
                if len(drafts) > 3:
                    st.caption(f"...è¿˜æœ‰ {len(drafts)-3} æ¡è‰ç¨¿ï¼Œä¸‹è½½å®Œæ•´åŒ…æŸ¥çœ‹")
                
                # å¯¼å‡ºæŒ‰é’®
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("ğŸ“¦ ä¿å­˜è‰ç¨¿åŒ…", use_container_width=True):
                        if not TEMPLATE_ENGINE_AVAILABLE or save_drafts_package is None:
                            st.error("âŒ ç”Ÿæˆæ¨¡å—æœªå°±ç»ªï¼Œæ— æ³•ä¿å­˜")
                        else:
                            package_path = save_drafts_package(drafts)
                            st.success(f"âœ… å·²ä¿å­˜åˆ°: {package_path}")
                
                with col2:
                    # æ‰“åŒ…ä¸º ZIP å¹¶ä¸‹è½½
                    if st.button("ğŸ“¥ ä¸‹è½½ ZIP", use_container_width=True):
                        import zipfile
                        import tempfile
                        
                        # åˆ›å»ºä¸´æ—¶ ZIP
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        zip_buffer = tempfile.NamedTemporaryFile(delete=False, suffix=".zip")
                        
                        with zipfile.ZipFile(zip_buffer.name, 'w', zipfile.ZIP_DEFLATED) as zf:
                            # drafts.jsonl
                            drafts_content = "\n".join([json.dumps(d, ensure_ascii=False) for d in drafts])
                            zf.writestr("drafts.jsonl", drafts_content)
                            
                            # README
                            readme = f"""è‰ç¨¿åŒ…

å…³é”®è¯: {st.session_state.get('package_keyword', 'N/A')}
ç”Ÿæˆæ•°é‡: {len(drafts)}
ç”Ÿæˆæ—¶é—´: {timestamp}

ä½¿ç”¨æ–¹æ³•ï¼š
1. æ‰“å¼€ drafts.jsonl
2. æ¯è¡Œæ˜¯ä¸€æ¡è‰ç¨¿ï¼ˆJSONæ ¼å¼ï¼‰
3. å¯æ ¹æ® account å­—æ®µåˆ†é…åˆ°ä¸åŒè´¦å·

å­—æ®µè¯´æ˜ï¼š
- title: æ ‡é¢˜
- body: æ­£æ–‡
- hashtags: æ¨èæ ‡ç­¾
- account: è´¦å·åˆ†é…
"""
                            zf.writestr("README.txt", readme)
                        
                        with open(zip_buffer.name, "rb") as f:
                            st.download_button(
                                label="ğŸ’¾ ä¸‹è½½è‰ç¨¿åŒ…",
                                data=f.read(),
                                file_name=f"drafts_{timestamp}.zip",
                                mime="application/zip",
                                use_container_width=True
                            )
                        
                        os.unlink(zip_buffer.name)


# === æ—¥å¿—å±•ç¤ºï¼ˆåº•éƒ¨ï¼‰===
with st.expander("ğŸ“‹ ç³»ç»Ÿæ—¥å¿—ï¼ˆæœ€è¿‘10æ¡ï¼‰", expanded=False):
    if st.session_state.logs:
        st.code("\n".join(st.session_state.logs[-10:]), language="log")
    else:
        st.info("æš‚æ— æ—¥å¿—")


# é¡µè„š
st.markdown("---")
st.caption("ğŸ“ AI Tools æ•°æ®æŒ–æ˜å·¥ä½œç«™ | Powered by MediaCrawler + NetworkX + Streamlit")
