# -*- coding: utf-8 -*-
"""
Packaging Utilities
æ‰“åŒ…å·¥å…·ï¼šç”Ÿæˆæäº¤åŒ… ZIP

åŠŸèƒ½ï¼š
- æ‰“åŒ…æ‰€æœ‰æ•°æ®ã€æŠ¥å‘Šã€æ–‡æ¡£
- ç”Ÿæˆ DELIVERY.md è¯´æ˜æ–‡æ¡£
- è¾“å‡ºè§„èŒƒçš„æäº¤åŒ…
"""
import os
import zipfile
import json
from datetime import datetime
from typing import List, Tuple


def create_submission_package(
    output_dir: str = "data/exports",
    project_root: str = "."
) -> Tuple[str, dict]:
    """
    åˆ›å»ºæäº¤åŒ… ZIP
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        project_root: é¡¹ç›®æ ¹ç›®å½•
        
    Returns:
        (zip_path, stats)
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_name = f"submission_{timestamp}.zip"
    zip_path = os.path.join(output_dir, zip_name)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "timestamp": timestamp,
        "files_included": [],
        "total_size_mb": 0.0
    }
    
    print("=" * 60)
    print("ğŸ“¦ å¼€å§‹åˆ›å»ºæäº¤åŒ…")
    print("=" * 60)
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
        # 1. æ•°æ®æ–‡ä»¶
        files_to_pack = [
            ("data/raw/annotations.jsonl", "data/raw/annotations.jsonl"),
            ("data/clean/annotations_clean.jsonl", "data/clean/annotations_clean.jsonl"),
            ("data/stats/cleaning_report.json", "data/stats/cleaning_report.json"),
            ("data/output/graph.html", "data/output/graph.html"),
        ]
        
        # 2. æ–‡æ¡£
        docs = [
            ("README_USAGE.md", "docs/README_USAGE.md"),
            ("QUICK_START.md", "docs/QUICK_START.md"),
        ]
        files_to_pack.extend(docs)
        
        # 3. æºä»£ç ï¼ˆå…³é”®æ–‡ä»¶ï¼‰
        code_files = [
            ("src/crawler/xhs_adapter.py", "src/crawler/xhs_adapter.py"),
            ("src/pipeline/cleaner.py", "src/pipeline/cleaner.py"),
            ("src/graph/builder.py", "src/graph/builder.py"),
            ("src/graph/analytics.py", "src/graph/analytics.py"),
            ("src/graph/visualizer.py", "src/graph/visualizer.py"),
            ("src/app/dashboard.py", "src/app/dashboard.py"),
        ]
        files_to_pack.extend(code_files)
        
        # æ‰“åŒ…æ–‡ä»¶
        for src, dst in files_to_pack:
            src_path = os.path.join(project_root, src)
            if os.path.exists(src_path):
                zf.write(src_path, dst)
                size = os.path.getsize(src_path)
                stats["files_included"].append(dst)
                stats["total_size_mb"] += size / 1024 / 1024
                print(f"  âœ“ {dst}")
            else:
                print(f"  âš  è·³è¿‡ï¼ˆä¸å­˜åœ¨ï¼‰: {src}")
        
        # 4. ç”Ÿæˆ DELIVERY.md
        delivery_content = generate_delivery_readme(stats)
        zf.writestr("DELIVERY.md", delivery_content)
        print(f"  âœ“ DELIVERY.md (è‡ªåŠ¨ç”Ÿæˆ)")
        
        # 5. æ‰“åŒ…æ—¥å¿—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if os.path.exists(os.path.join(project_root, "logs/app.log")):
            zf.write("logs/app.log", "logs/app.log")
            print(f"  âœ“ logs/app.log")
    
    print("=" * 60)
    print(f"âœ… æäº¤åŒ…å·²ç”Ÿæˆ")
    print(f"  æ–‡ä»¶: {zip_path}")
    print(f"  å¤§å°: {stats['total_size_mb']:.2f} MB")
    print(f"  åŒ…å«: {len(stats['files_included'])} ä¸ªæ–‡ä»¶")
    print("=" * 60)
    
    return zip_path, stats


def generate_delivery_readme(stats: dict) -> str:
    """
    ç”Ÿæˆ DELIVERY.md äº¤ä»˜è¯´æ˜æ–‡æ¡£
    
    Args:
        stats: æ‰“åŒ…ç»Ÿè®¡ä¿¡æ¯
        
    Returns:
        str: Markdown å†…å®¹
    """
    content = f"""# æäº¤åŒ…è¯´æ˜æ–‡æ¡£

## ğŸ“¦ æäº¤ä¿¡æ¯

- **ç”Ÿæˆæ—¶é—´**: {stats.get('timestamp', 'N/A')}
- **åŒ…å«æ–‡ä»¶**: {len(stats.get('files_included', []))} ä¸ª
- **æ€»å¤§å°**: {stats.get('total_size_mb', 0):.2f} MB

---

## ğŸ“ æ–‡ä»¶æ¸…å•

### æ•°æ®æ–‡ä»¶

- `data/raw/annotations.jsonl` - åŸå§‹çˆ¬å–æ•°æ®
- `data/clean/annotations_clean.jsonl` - æ¸…æ´—åæ•°æ®
- `data/stats/cleaning_report.json` - æ¸…æ´—ç»Ÿè®¡æŠ¥å‘Š
- `data/output/graph.html` - äº¤äº’å¼æ ‡ç­¾å…±ç°å›¾è°±

### æ–‡æ¡£

- `docs/README_USAGE.md` - å®Œæ•´ä½¿ç”¨æŒ‡å—
- `docs/QUICK_START.md` - å¿«é€Ÿå¯åŠ¨æ¸…å•
- `DELIVERY.md` - æœ¬æ–‡æ¡£

### æºä»£ç 

- `src/crawler/xhs_adapter.py` - Stage-1: çˆ¬è™«é€‚é…å™¨
- `src/pipeline/cleaner.py` - Stage-2: æ•°æ®æ¸…æ´—
- `src/graph/builder.py` - Stage-3: å›¾æ„å»º
- `src/graph/analytics.py` - Stage-3: PageRank/è¶‹åŠ¿åˆ†æ
- `src/graph/visualizer.py` - Stage-3: å¯è§†åŒ–
- `src/app/dashboard.py` - Stage-4: Streamlit å·¥ä½œç«™

---

## ğŸš€ å¿«é€Ÿå¤ç°

### ç¯å¢ƒè¦æ±‚

- Python >= 3.11
- ä¾èµ–ç®¡ç†: uvï¼ˆæ¨èï¼‰æˆ– pip

### å®‰è£…ä¾èµ–

```bash
# ä½¿ç”¨ uvï¼ˆæ¨èï¼‰
uv sync

# æˆ–ä½¿ç”¨ pip
pip install -r requirements.txt
```

### å¯åŠ¨å·¥ä½œç«™

```bash
streamlit run src/app/dashboard.py
```

æµè§ˆå™¨æ‰“å¼€ `http://localhost:8501`ï¼Œç‚¹å‡» **Mine** æŒ‰é’®å³å¯æŸ¥çœ‹ç»“æœã€‚

---

## âœ… éªŒæ”¶æ ‡å‡†

### Stage-1: çˆ¬è™«
- [x] èƒ½ç¨³å®šçˆ¬å–å°çº¢ä¹¦æ•°æ®
- [x] è¾“å‡ºåˆ° `data/raw/annotations.jsonl`
- [x] å­—æ®µå®Œæ•´ï¼ˆ9ä¸ªå¿…éœ€å­—æ®µï¼‰

### Stage-2: æ•°æ®æ¸…æ´—
- [x] æ¸…æ´—è§„åˆ™å®æ–½ï¼ˆæè¿°é•¿åº¦ã€æ ‡ç­¾å¤„ç†ã€å»é‡ï¼‰
- [x] ç”Ÿæˆæ¸…æ´—æŠ¥å‘Š
- [x] é€šè¿‡ç‡ > 80%

### Stage-3: å›¾è°±æŒ–æ˜
- [x] æ ‡ç­¾å…±ç°å›¾æ„å»º
- [x] PageRank Top æ¦œå•
- [x] Rising Edges è¶‹åŠ¿å‘ç°ï¼ˆå¸¦çª—å£æ ·æœ¬æ•°å±•ç¤ºï¼‰
- [x] äº¤äº’å¼ HTML å›¾è°±

### Stage-4: Streamlit å·¥ä½œç«™
- [x] å•é¡µ Dashboard
- [x] æ•°æ®æºåˆ‡æ¢ï¼ˆå…œåº• Demo Modeï¼‰
- [x] ä¸€é”®æŒ–æ˜åŠŸèƒ½
- [x] å¯è§†åŒ–å±•ç¤ºï¼ˆå›¾è°±+æ¦œå•+æ ·æœ¬ï¼‰

---

## ğŸ“Š æ•°æ®ç»Ÿè®¡

æ ¹æ® `data/stats/cleaning_report.json`ï¼š

- **åŸå§‹æ•°æ®**: {raw_count} æ¡
- **æ¸…æ´—å**: {clean_count} æ¡
- **é€šè¿‡ç‡**: {pass_rate}%

---

## ğŸ¯ æ ¸å¿ƒåˆ›æ–°ç‚¹

1. **æ ‡ç­¾å…±ç°å›¾æŒ–æ˜**: å‘ç°æ ¸å¿ƒè¯é¢˜å’Œçƒ­é—¨ç»„åˆ
2. **æ—¶é—´çª—å£è¶‹åŠ¿**: Rising Edges å‘ç°"æ­£åœ¨å˜çƒ­"çš„è¯é¢˜ç»„åˆ
3. **å·¥ç¨‹ç¨³å®šæ€§**: Demo Mode + å®¹é”™å¤„ç† + æ—¥å¿—å¯è§‚æµ‹
4. **å¯å¤ç°æ€§**: å•ä¸€æ•°æ®æºï¼ˆJSONLï¼‰+ å®Œæ•´æ—¥å¿— + ä¸€é”®æ‰“åŒ…

---

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒï¼š
- å®Œæ•´æ–‡æ¡£ï¼š`docs/README_USAGE.md`
- å¿«é€ŸæŒ‡å—ï¼š`docs/QUICK_START.md`
- æºç æ³¨é‡Šï¼šå„æ¨¡å—æ–‡ä»¶

---

**ğŸ“ AI Tools æ•°æ®æŒ–æ˜å·¥ä½œç«™ | Stage 1-4 Complete**

ç”Ÿæˆæ—¶é—´: {stats.get('timestamp', 'N/A')}
"""
    
    # å¦‚æœæœ‰æ¸…æ´—æŠ¥å‘Šï¼Œæ’å…¥çœŸå®æ•°æ®
    try:
        with open("data/stats/cleaning_report.json", "r", encoding="utf-8") as f:
            report = json.load(f)
            content = content.replace("{raw_count}", str(report.get("raw_count", "N/A")))
            content = content.replace("{clean_count}", str(report.get("clean_count", "N/A")))
            content = content.replace("{pass_rate}", str(report.get("pass_rate", "N/A")))
    except:
        pass
    
    return content


def deduplicate_jsonl(items: List[dict]) -> List[dict]:
    """
    åŸºäº item_id å»é‡
    
    Args:
        items: JSON å¯¹è±¡åˆ—è¡¨
        
    Returns:
        å»é‡åçš„åˆ—è¡¨
    """
    seen_ids = set()
    unique_items = []
    
    for item in items:
        item_id = item.get("item_id")
        if item_id and item_id not in seen_ids:
            unique_items.append(item)
            seen_ids.add(item_id)
    
    return unique_items


def merge_jsonl_files(file_paths: List[str], output_path: str) -> int:
    """
    åˆå¹¶å¤šä¸ª JSONL æ–‡ä»¶å¹¶å»é‡
    
    Args:
        file_paths: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Returns:
        åˆå¹¶åçš„æ•°æ®æ¡æ•°
    """
    all_items = []
    
    for path in file_paths:
        if not os.path.exists(path):
            continue
        
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    all_items.append(json.loads(line.strip()))
                except json.JSONDecodeError:
                    continue
    
    # å»é‡
    unique_items = deduplicate_jsonl(all_items)
    
    # ä¿å­˜
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        for item in unique_items:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    return len(unique_items)
