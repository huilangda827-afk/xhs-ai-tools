# XHS 基础爬虫测试指南

## 🎯 目标

验证能否从小红书抓取数据并按标准 schema 写入 `data/annotations.jsonl`

---

## 🚀 快速开始

### 1. 确保依赖已安装

```bash
pip install -r requirements.txt
```

### 2. 运行测试脚本

```bash
python scripts/test_crawl_raw.py
```

### 3. 首次运行（需要登录）

- **如果已有登录状态**：自动复用，无需操作
- **如果首次运行**：
  1. 浏览器会自动打开
  2. 扫描二维码登录小红书
  3. 登录成功后，状态会自动保存到 `browser_data/xhs_user_data_dir/`
  4. 后续运行无需再次登录

### 4. 预期输出

```
==================================================
🚀 XHS 基础爬虫测试
==================================================

📌 搜索关键词: AI工具
📌 目标数量: 5 条笔记
📌 输出文件: data/annotations.jsonl

⏳ 开始爬取...
--------------------------------------------------

[爬取日志...]

==================================================
✅ 爬取完成！成功保存 5 条笔记
📄 输出文件: data/annotations.jsonl
==================================================

📋 数据预览（前 2 条）:
--------------------------------------------------

[笔记 1]
  标题: AI工具推荐：10个提升效率的神器...
  描述: 分享我最近在用的几个AI工具...
  标签: ['AI工具', '效率', '办公']
  图片数: 3
  时间: 2026-01-15T10:30:00

...
```

---

## 📋 输出格式

每条数据包含以下字段（缺失时用空值/None，不会报错）：

```json
{
  "item_id": "笔记ID",
  "source": "xhs",
  "url": "完整URL或null",
  "time": "ISO格式时间或null",
  "title": "标题",
  "desc": "描述",
  "tags": ["标签1", "标签2"],
  "images": ["图片URL1", "图片URL2"]
}
```

---

## ⚙️ 自定义配置

编辑 `scripts/test_crawl_raw.py`：

```python
# 修改搜索关键词
keyword = "DeepSeek"  # 改为其他关键词

# 修改抓取数量
max_notes = 10  # 抓取 10 条笔记
```

---

## 🔧 故障排查

### 问题 1：浏览器无法打开
**解决**：确保系统已安装 Chrome/Edge，并检查 `config/base_config.py` 中的 `ENABLE_CDP_MODE` 配置

### 问题 2：登录失败
**解决**：
1. 删除 `browser_data/xhs_user_data_dir/` 目录
2. 重新运行脚本
3. 手动扫码登录

### 问题 3：抓取数量少于预期
**可能原因**：
- 关键词搜索结果不足
- 被平台限流（需等待或更换关键词）

**解决**：
- 尝试更换关键词（如 "DeepSeek", "ChatGPT"）
- 调整 `config/base_config.py` 中的 `CRAWLER_MAX_SLEEP_SEC` 增加间隔

### 问题 4：某些字段为空
**这是正常的！** 系统已设置容错机制：
- `tags` 为空 → `[]`
- `time` 为空 → `null`
- `images` 为空 → `[]`
- 不会中断爬取

---

## 📁 文件说明

```
scripts/
  ├── test_crawl_raw.py      # 测试脚本（运行入口）
  └── README_TEST.md         # 本说明文档

src/
  └── crawler/
      ├── __init__.py        # 包初始化
      └── xhs_adapter.py     # 核心适配器（包装 MediaCrawler）

data/
  └── annotations.jsonl      # 输出数据（每次运行覆盖）
```

---

## ✅ 验证成功标志

1. ✓ `data/annotations.jsonl` 文件存在
2. ✓ 包含 5 条 JSON 行（可用 `jq` 或文本编辑器查看）
3. ✓ 每行包含所有必需字段（即使某些为空）
4. ✓ `source` 字段固定为 `"xhs"`
5. ✓ 无程序崩溃或中断

---

## 🎉 下一步

测试成功后，可以：
1. 增加抓取数量（修改 `max_notes`）
2. 尝试不同关键词
3. 进入 Stage-2：添加质量评分、图片下载
4. 进入 Stage-3：构建标签共现图、趋势分析

---

**任何问题？** 检查日志或提 issue 💬
